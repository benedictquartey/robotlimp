{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limp.utils.gen_utils import ltl2dfa, llm4tl, referent_verification, task_structure_verification, get_spatial_referents\n",
    "from osg.utils.general_utils import load_data\n",
    "from osg.vlm_library import vlm_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fldr=f\"results/\"\n",
    "\n",
    "data_format = \"r3d\"     \n",
    "data_path = \"../data/sample.r3d\"   \n",
    "\n",
    "vlm_instance = vlm_library(vl_model=\"owl_vit\", data_src=data_format, seg_model= \"mobile_sam\", tmp_fldr=tmp_fldr) \n",
    "\n",
    "env_pointcloud, observations_graph = load_data(data_path,                  # path to data\n",
    "                                               data_format,                # data format [robot: robot collected data / r3d: iphone collected data (via Record3d app)]\n",
    "                                               tmp_fldr,                   # temp folder to store execution artefacts\n",
    "                                               pcd_downsample=False,       # flag to downsample generated pointcloud\n",
    "                                               compression_percentage=60   # reduce rgbd observations used to create map by xx% \n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language to  Ltl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction for github: take the bowl next to the xxx to the sink, and bring me, the microwave is acting up so try to not go near it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go bring the plush toy between the sofa and the television to the couch, my cat is on the brown table so please dont pass near the table when you are returning with the toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demo instruction: Bring the green plush toy to the whiteboard in front of it\n",
    "\n",
    "print(f\"\\n*************************************************************************\\nInstruction Following\\n*************************************************************************\")\n",
    "input_lang_instruction = input(\"Enter the natural language description of the task: \")\n",
    "\n",
    "in_context_examples = \"limp/language/temporal_logic/ltl_datasets/efficient-eng-2-ltl-droneplanning\"\n",
    "in_context_count    = 10\n",
    "lang2ltl_path       = \"limp/language/temporal_logic/embedding_cache/small-droneplanning_lang2ltl.pkl\" \n",
    "lang2embedding_path = \"limp/language/temporal_logic/embedding_cache/small-droneplanning_lang2embeddings.pkl\"\n",
    "\n",
    "api_choice = \"openai\" #googleai / openai\n",
    "print(f'\\nInput instruction: \"{input_lang_instruction}\"')\n",
    "print(\"Running Language Instruction Module ...\")\n",
    "\n",
    "strategy_choice=\"two_stage_similar_embedding\"\n",
    "# strategy_choice=\"two_stage_random_embedding\"\n",
    "# strategy_choice=\"single_stage\"\n",
    "\n",
    "encoding_map, response_ltl, spot_ltl, llm_response_history= llm4tl(api_choice, \n",
    "                                                                   input_lang_instruction, \n",
    "                                                                   in_context_examples, \n",
    "                                                                   lang2embedding_path, \n",
    "                                                                   lang2ltl_path, \n",
    "                                                                   in_context_count, \n",
    "                                                                   enable_prints=False,\n",
    "                                                                   strategy=strategy_choice)\n",
    "\n",
    "original_encoding_map, original_response_ltl, original_spot_ltl, original_llm_response_history = encoding_map, response_ltl, spot_ltl, llm_response_history \n",
    "\n",
    "print(\"Spotify predicate encoding map: \", encoding_map)\n",
    "print(\"Response LTL formula: \", response_ltl)\n",
    "print(\"Cleaned LTL formula: \", spot_ltl,\"\\n\")\n",
    "\n",
    "display(spot_ltl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Symbol Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referent verification\n",
    "encoding_map, response_ltl, spot_ltl, llm_response_history  = referent_verification(input_lang_instruction, encoding_map, response_ltl, spot_ltl, llm_response_history, strategy_choice, api_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task structure verification\n",
    "encoding_map, response_ltl, spot_ltl, llm_response_history, selected_dfa_path  = task_structure_verification(input_lang_instruction, encoding_map, response_ltl, spot_ltl, llm_response_history, strategy_choice, api_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original and verified results\n",
    "print(\"*****************************************\\nOriginal Results\\n*****************************************\")\n",
    "print(\"Original encoded formula: \",original_spot_ltl)\n",
    "print(\"Original encoding map: \",original_encoding_map)\n",
    "print(\"*****************************************\\nAfter Verification\\n*****************************************\")\n",
    "print(\"Verified encoded formula: \",spot_ltl)\n",
    "print(\"Verified encoding map: \",encoding_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct task dfa from ltl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing task dfa from ltl formula\n",
    "task_dfa, dfa_graph = ltl2dfa(encoding_map, spot_ltl, visualize_details=True, show_diagram=True, show_labels=True, path=selected_dfa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground referents and filter instances via spatial constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract spatial information\n",
    "referent_spatial_details = get_spatial_referents(encoding_map)\n",
    "print(\"referent_spatial_details: \",referent_spatial_details,\"\\n\")\n",
    "\n",
    "## Spatial grounding\n",
    "relevant_element_details = vlm_instance.spatial_grounding(observations_graph, referent_spatial_details, visualize=True, use_segmentation=True, multiprocessing=True, workers=3)\n",
    "\n",
    "print(\"Referents after spatial constraint filtering:\",len(relevant_element_details))\n",
    "#for all relevant elements print their ids\n",
    "print(f\"\\nFiltered elements \\n\",[element['mask_id'] for element in relevant_element_details])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select Robot Start Point in Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limp.planner.multi_level_planner import generate_obstacle_map\n",
    "from limp.utils.fmt_utils import plot_map_with_points\n",
    "%matplotlib widget\n",
    "\n",
    "resolution = 0.01\n",
    "h_min_bottom = -1   #experiment with values to filter out floor\n",
    "h_max_top = 1       #experiment with values to filter out ceiling\n",
    "obstacle_map, _, map_min_bound, map_max_bound = generate_obstacle_map(env_pointcloud, None, resolution, h_min_bottom,  h_max_top)\n",
    "\n",
    "## Visually getting start point from map\n",
    "clicked_points = plot_map_with_points(obstacle_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressive motion planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limp.planner.multi_level_planner import progressive_motion_planner\n",
    "\n",
    "robot_motion_type            = \"2D\"                   # determines if planing space is 2D or 3D\n",
    "z_height_2d                  = 0                      # height of 2D planning space\n",
    "step_factor                  = 40                     # determines density of generated visual demarcations of regions of interest. Adjust for denser or sparser points\n",
    "goal_sampling_percentage     = 15                     # percentage of goal points to sample from the goal region to make exhaustive motion planning tractable\n",
    "use_heuristic_flag           = True                   # determines if we use modified version of FMT* with cost to goal heuristic \n",
    "visualize_flag               = True                   # determines if we visualize the computed motion plan\n",
    "obstacle_map_resolution      = 0.01                   # determines the resolution of the obstacle map\n",
    "filter_h_min_bottom          = h_min_bottom           # height to filter out pointcloud points below this height (meters) [Floor]      \n",
    "filter_h_max_top             = h_max_top              # height to filter out pointcloud points above this height (meters) [Roof] #see doors: 0.7 || Old value:-0.15\n",
    "nearness_threshold           = 0.5                      # determines the meaning of nearness of planning space demarcation (meters)\n",
    "start_point                  = clicked_points[-1]     # robot start location\n",
    "show_color_bars_flag         = False\n",
    "\n",
    "computed_plan = progressive_motion_planner(start_point, \n",
    "                                           task_dfa, \n",
    "                                           dfa_graph, \n",
    "                                           env_pointcloud, \n",
    "                                           relevant_element_details, \n",
    "                                           encoding_map, \n",
    "                                           nearness_threshold, \n",
    "                                           obstacle_map_resolution, \n",
    "                                           filter_h_min_bottom, \n",
    "                                           filter_h_max_top, \n",
    "                                           robot_motion_type, \n",
    "                                           height_2d=z_height_2d, \n",
    "                                           stepfactor=step_factor, \n",
    "                                           use_heuristic=use_heuristic_flag,\n",
    "                                           visualize=visualize_flag,\n",
    "                                           tmp_fldr=tmp_fldr,\n",
    "                                           goal_sample_percentage=goal_sampling_percentage,\n",
    "                                           show_color_bars=show_color_bars_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Task and Motion Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_plan['world_plan'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlgmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
